---
title: "ReadMe - Pamehac"
author: "Keith Lewis"
date: "2024-08-07"
output: 
  html_document:
    toc: true
    toc_float: true
bibliography: refs/references.bib
---

```{r, import data, echo = F, message = F}
df_all <- read.csv("data_derived/df_all.csv")
df_sum <- read.csv("data_derived/df_sum.csv")
df_tab_T1 <- read.csv("data_derived/df_tab_T1.csv")
df_sum_subset <- read.csv("data_derived/spc_example.csv")
df_a <- read.csv("data_derived/df_a3.csv")
df_out <- read.csv("data_derived/FSA_output.csv")
df_tab3 <- read.csv("data_derived/df_tab3.csv")

library(ggplot2)
library(dplyr)
library(tidyr)
```

# Introduction
This file is simply a record of issues that I have had with the Pamehac analysis that was started by Kristin Loughlin and is a continuation of work begun by Dave Scruton.   See ReadMe files from Rose Blanche and Seal Cove for additional info.

## Project organization

Got a zip file from Kristin Loughlin with no folders but files as follows:  
- Carle Strub Estimates    
- data in various formats  
- some R files for various plots  

I reorganized for my own needs.  I have recreated all the figures although there are still questions, especially about the confidence intervals so basically, I learved how to do Carle Strub and many other estimates of abundance.


# Pamehac Brook
Pamehac Brook is a tributary of the largest river on the island of Newfoundland, the Exploits River (48o53 ’ N; -56 o07’ W; Figure 6, Figure 7A).  The Exploits River also has the highest Atlantic salmon run on the island.  Pamehac Brook was modified in the 1970s to enable the transport of logs to a pulp and paper mill.  There were several control dams constructed in the upper watershed of the brook and the headwaters were diverted to create a 1.5 km long high gradient diversion stream emptying into the Exploits River.  The diversion impacted water flows in 12 kilometers of the original stream including completely dewatering the stream from the diversion down to the first inflow tributary.  After the diversion was constructed, flows at the mouth of Pamehac Brook were decreased by 36% (Scruton et al. 1998).  Good quality salmon and trout habitat was destroyed due to both the dewatering and flow reductions to the original stream and obstructions to salmon migration in the diversion canal prevented anadromous salmon from even reaching the upper watershed of Pamehac Brook.  To further facilitate log driving, portions of the upper watershed above the diversion were channelized, reducing the quality of habitat and increasing flows.  The banks of the diversion stream were unstable and led to issues with erosion and sedimentation as well.  In the 1980s, log driving activities ended but the infrastructure on Pamehac Brook remained in place until 1990 when efforts were undertaken to improve both habitat and anadromous salmon accessibility in the brook.  Several organizations including Environment Resources Management Association (ERMA, a conservation group), Abitibi Price (owner of the pulp and paper mill), and Fisheries and Oceans Canada spearheaded the restoration efforts.  Two control dams and a collapsed culvert on the brook were replaced with new bridges and the diversion was removed allowing natural flows to be restored to the stream for the first time in more than two decades (Figure 7C).  The newly rewatered stream bed was surveyed for obstructions such as debris and beaver dams and these were removed.  
Further information on the study area and these restoration efforts can be found in Scruton et al. (1998).  The main species of interest for this restoration project was Atlantic salmon given the importance of the Exploits as a salmon river.  In the first year of monitoring, there was very little change in salmon density in Pamehac Brook.   The density of young of the year increased significantly in 1992 as 42,000 fry were stocked in two locations the stream.  This stocking was not continued as the goal of the project was to examine natural recolonization of the stream by salmon.  Stocked fry would have exited the stream by 1996 as salmon smolts and density estimates from that year are considered more indicative of the natural density in the stream.  



## Missing values 
**SOME OF THIS HAS CHANGED**
2024-10-23: checked that df_all, df_sum, df_tab1, and out all add up.  df_all and df_sum are fine.  df_tab1 can differ from the less refined dataframes because df_tab1 includes only Sweep 1 to Sweep3, i.e., it eliminates sites with only captures on Sweep 1 but the last line (filter) eliminated sites with with captures on Sweep 1 (or 2 or 3) & 4 or 5 (all three have captures on sweep 3).  So, I added df_tab2 for the tabulations because it represents all 140 sites.  Also, df_tab_T presents the data as in the table below (see tab "samples and catches by year"). There are also 17 sites with zeros in the first sweep.

I think that this is OK for analytical purposes but there may be troubles with the tabulation.

I also changed the north (latitude) on "Pam5a bottom" from 8.894....to 48.894 as this is clearly a typo.

## Scruton
Using R, I have been unable to replicate the Scruton et al. 1998 numbers in Figure 2 and 3. However, when I use Excel and the Pamehac_[year]_by_type.csv (did 1990 and 1996), a near perfect match occurs for all of the Above stations when I count them, divide by Area and multiply by 100 (#/A)*100.  This also seems to work for the Salmon on the Below sites but not the Trout (no idea why - I tried with ).  

I think SCruton pooled the above and below sites and then scaled them by area.  However, this did not work at all for biomass.  But it appears that some of the tye

I checked Pamehac_1996_by_type.csv by Pamehac1996_by_station.csv and found a discrepancy in the counts - it seems that in the by_type.csv file, that a lot of BTYOY have been changed to BT.  No idea why this is the case but probably explains why I got different numbers.

But I cannot figure out how he got his biomass estimates as they are often much lower than mine.  Tried raw numbers/area and mean_weight*raw_number/area - same number.    Also, did with estimated No.  Sometimes off by a factor of 10 in 1991.  1996 is closer but still far off the mark.

In Jan 2025, I found a couple of things.  First, I tried: - scaling by area, 
- using FSA numbers scaled by area, 
- the previous with zeros,  
- the previous with zeros with 5 and 8 averaged  
- see back of Scurton et al. paper for notes (2025-01-21)  

None of the above match Scruton.  

Found the file "Comparison_Pamehac_Scrutonetal1998_paper.xlsx.  This is a comparison of the estimates from Scruton and Loughlin (using Cote's code) and they differ, sometimes a lot.  There is a summary table that I can reproduce but as there are no codes, its not clear where Kristin got it and the numbers in the summary table do not match the less summarized data.  


So there is a discrepancy between what Clakre and Curtis remember and what is in Scruton et al [-@scruton1998pamehac], i.e., the number of passes and whether the fish were driven to the nets. Spoke to Keith Clarke about this on 2024-10-25.  He said he's not surprised that there is a discrepancy and that differences in methodology may have been glossed over.  But he's sure that Bourgeois had something to do with this.  

I also found that the salmonid estimates in Fig 2 and 3 are supposed to be for juvenile but for density, the values in BT/AS/YOY do not equate to the salmonids figures.  I also cannot replicate his results in anyway.  This is especially worrisome for BT density - SCruton seems to have the raw and unscaled values plotted but not for the other years.  I spoke to Clarke about this and apparently, this paper was for a conference and he was not surprised if mistakes are made.  So, he said to proceed and not worry about replicating Scruton's values.  STill need to talk to Kristin about this.

# Data {.tabset}

## Notes
See the Rose Blanche code for explanations on what I think the AMEC code does and my concerns, i.e., how to use the delta method for these types of data as well as how the initial summaries were naively done.  
BUT......the really, really, really good thing for Pamehac is that there is a file "output_pamehac_by_station.csv" that has all of the data as derived from the AMEC functions but in raw form!!!  So, I have created code in Pam_data_new.R to bring in all the files together.  From there, one can do any type of depletion analysis.

## Files


## Pipeline
SD = sweep disaggreated, SA = sweep aggregated
- ls_pam: list of electrofishing files in the Pamehac directory; SD  
- df_all: take ls_pam and make a dataframe; clean it up; SD  
- df_sum: from df_all, calculate biomass and abundance for each sweep; SD  
- df_grid: create a grid with all possible rows for Year, Statoin, Species, and Sweep - this is to take care of the "no line when no fish of a given species is caught" problem; SD  
df_grid1: take out the structural zeros; SD  
- df_all1: join df_grid1 with df_sum to make a complete data set; SD  
- df_all2: join df_all1 with df_stn_tag (max sweeps per year) to remove extra sweeps; replace NA's with zeros; calculate spc; SD  
- df_a: Sweeps < 3; sum abundance and biomass; SA; df_a2: add area; df_a3: add location  
- df_a1: this may be a relic  
- df_tab1: tabular format of abundance and biomass, Sweep <= 3; SA  
- df_tab2: tabular format of abundance and biomass, all Sweeps; SA  
- df_tab3: tabular format of abundance and biomass, all Sweeps whether fish or not
- df_tab_T: sum by species and year; tabular format
- df_tab_T1: as above but with sweeps  

## remove Stations
Sites with missing years - see df_all.xlsx.  

9 and 5B are only sampled in 1991/1992 and 1991 respectively.  There are no other 9 stations but there is 5 and 5A.  Conclusion - eliminate 9 and 5B.  

By the same logic, I should drop 8A which was only done in 2 years.  

5A was done in three "after" years but so was Stn 3. But Stn 3 has no A or B sites - this argues for keeping it.  What about 5A?

I went through all the models again without 5A and 8A - long story, short, there's not much difference.  I changed the data set used for analysis (df_a) in Pam_analysis.R and compared the point estimate and the CIs for each model.  Many were virtually identical.  BTY density was a bit more different for the interaction but in no case would it change the decision, i.e., accept/reject hypothesis.

## map
```{r, map, echo = F}
tmp <- read.csv("data/waypoints.csv")
library(leaflet)
leaflet()%>%
   setView(lng = -56.07,lat=48.91,zoom=11)%>%addTiles()%>%
   addCircleMarkers(data=tmp, lng=tmp$west, lat=tmp$north,
                    popup = ~site,
                    label = ~site,
                    radius = 6,
                    color = 'black',
                    fillColor = 'blue',
                    stroke = TRUE,
                    weight = 1,
                    fillOpacity = 0.5) 

```

# Abundance Estimates
## Why are some CIs wide
One thing that I noticed though is that the CIs for some of the estimates are very narrow while others are extremely wide.  Why are they so wide?
Several reasons:  
1. lack of data  - if only a few fish, often get NA  
2. lack of depletion  

On a more minor note:  
See below for assumptions behind depletion method and Pam_data_new for "Sum of Previous Catch" (spc) and Pam_abun_bio.R for Goodness of fit (GF) tests.  

Why does the AMEC function not return its output aside from a graph and a csv file???

## literature review {.tabset}

After reviewing the early (~ pre-2000) literature on depletion estimates, it became clear that the CS method is best but there is a whole new literature composed of N-mixture models and Bayesian approaches.  

### early
After extensive exploration, it is clear that there is no "right" way to model abundance for all of the species:year:stations.  This is consistent with Riley and Fausch [-@riley1992underestimation] who found "decreasing capture probability was a frequent problem when estimating trout abundance in small Colorado mountain streams".  Their streams seem comparable to Seal Cove.

Cowx [-@cowx1983review] showed that Carle Strub is the more reliable method than earlier approaches such as Zippin. Has the equations, much like the tutorial by Lockwood [-@lockwood2000stream].  Also lists 5 assumptions of depletion methods.  Gives the downsides of all methods except CS.

Gatz and Loar [-@gatz1988petersen] show that results can be improved by combining Petersen with removal methods in a reasonable amount of time, i.e., 2 days

Riley and Fausch [-@riley1992underestimation] found that 2 and 3-pass estimates result in underestimates but 3 is better than 2.  Suggest that its due to decreasing p but not complexity of habitat but I found their statistical methods wanting on this front.  But the power of chi-square tests is low below population size of 200. Therefore, its important to look at chi-square and spc together.  

### later (post 2000)
Heimbuch et al. [-@doublepass] use a double pass approach but I think that this is a bit like Hedger - they use all sites to get an approximation of catchability - I don't think this is a good idea.

Reid [-@reid2009evaluation] - single pass seems to work OK as an index; is this OK for GLMMs?
Reid 2011 - eels

Hedger et al. [-@hedger2013improving] found that - summarize constant $\rho$ based on first pass and variable p based on 

Matechou 2016 (not in refs) - open models for removal - cool but to be read later

Hanks 2018 - differences in power bewteen 1- and 3-passses to detect annual population declines diminishes as more sites are surveyed and nearly identical when 5+ sites are surveyed but methods are very correlated. Focus is on trend detection.  But single pass is an index, not absolute abundance. can't compare with other areas with different capture probabilities.  

Hedger 2018 - capture probability estimates depend on number of passes and decline with each pass with effect depending on size class - probably not a surprise. Ergo, 3-pass will underestimate.

Link 2018 (not in refs) robust design and removal.  Looks cool but may not apply and will be hard to implement

### Bayesian
Working on the Richter approach - it seems fine conceptually but suffers from the same problem as package unmarked, i.e., it all comes down to q.  When I set the q, it seems to come up with reasonable results. See Paul Regular below.   

So, I think I found a clue to why the population estimates are so high.  If you do the length approach advocated by Richter, then there are some lengths where there is no depletion and this would suggest that there are lots more fish.  But for very low n(n <=3, the q for the second length is still very, very low).

Reid and Haxton [-@reid2017backpack] used a Bayesian approach to model sampling data, site occupancy, and detection probability.  I think that this is a state-space (N-mixture) approach because they model occupancy and then use that in the model of observations. No covariates to model detection. They also looked at time spent fishing.

Richter et al [-@richter2022correcting] use a Bayesian method to correct for size bias.  There is a lot of merit in this approach - i'm just not sure if it really makes that much difference in the end.  They also have some rules for when to stop sampling but also which reaches to include in the data set.  But it seems like they are just doing a standard depletion model with q as catchability - they are not trying to model q in anyway through a regression.  Explored this in detail - it still gives wonky estimates because q can be very low.  This may or may not be because of non-declining catches or low sample sizes.  Don't understand how q is estimated based on site and size, especially for all three removal periods. https://github.com/ianarichter/bayesian-removal-model

### N-mixture/unmarked
However: there is package unmarked (Finske and Chandler 2011) and the Bayesian spinoffs.  
"With the exception of the `open' population models (colext, gmultmix, and pcountOpen), all fitting functions use a double right-hand sided formula syntax representing the hierarchical model structure. Specifically, covariates
affecting the **detection process** are specified following the first tilde, and covariates of the **state process** follow the second tilde. **No left-hand side** of the formula is specified because the unmarked Frame defines the response variable uniquely as the y slot."
"As described in Section 2.2, covariates of detection
are on the logit-scale and covariates of abundance are on the log-scale for the repeated count
model."

Conclusion on umarked:
Spent a lot of time on package unmarked but there are very few tutorials and everything is dependent on the 'q' estimate which can result in wildly non-sensical estimates

Lamothe et al. 2023 - simulations on occupancy and n-mixture; occupancy models are slightly better.  Fig.1 is great - both are state-space models but occupancy models occupancy before observed number while n-mixture models true abundance before observed abundance. Finish reading. They used gmultmix - need to figure out why this and not multinomPois
https://cran.rstudio.com/web/packages/unmarked/vignettes/cap-recap.html


### tutorials/reviews
Lockwood [-@lockwood2000stream] is a great tutorial on recapture and depletion methods.  See also 2013 fishR tutorial but Derek Ogle.

Lecture10: https://rushinglab.github.io/WILD6900/articles/Lecture10/Lecture10.html#4

Rivera 2021 - a review: very general but with no key insights.  It does provide a nice history of the field.  Almost all depletion can be treated as a CMR analysis but with recapture set to zero.

### Scruton and Gibson
See Clarke and Curtis tab.  They advised me to look up Scruton and Gibson [-@scruton1995quantitative] which is an overview electrofishing in NL based on 1993 workshop.

- Summary of Papers  
	- Gibson - Freshwaer River, NE Trepassey River, Highlands -  move upstream (Gibson references)  
	- Bourgeois -  Lloyds Section III and II, King Georve IV Lake - move upstream in 1987-88 but then move downstream  
	- Scruton - West Salmon River, Dog Pond, Seal Cove - move upstream - calls it the fixed effort (successive) removal method; thanks Bourgois for advice but not for field work  
	- Mullins and Lowe - Humber River, Western Arm Brook, Pinchgut Brook - move downstream  
	- Warren advocates for Bayesian approach!!!!!!  
	
- Note  	
  - Pamehac is mentioned three times by Scruton but not in relation to electrofishing, at least not directly
  - Scruton et al. 1998 references this paper for the methods






## What have I learned:
Assumptions [@cowx1983review]: 3 standard and two more:    
    - closed population -> this should always be met in NL  
    - probability of capture is the same for all individuals    
    - probability of capture (catchability) remains the same for each capture occasion - unlikely to be true often  
    - non-standard: the CPUE must significantly reduce the population size  
    - non-standard: the population was not be so large that catching of one member interferes with the capture of another  

What about this??? 2. fishing effort must remain constant for each capture occasion - assume that this is the case but see Clarke and Curtis - see Notes from them - this is definitely not the case for Pamehac

- Can do depletion on open and closed  
- Samples > 30 total are required  
- Goodness-of-fit power isn't great with < 200 fish  
- Have to capture most of the fish (but this begs the question of why employ these approaches to begin with)
- Assumptions are often violated  

Lots of passes means less sites, less power, and less generalization.  1 pass means more sites but less accuracy of the estimate.  I decided that it was time to reach out (see Consults).  I also want to know if this really makes any difference or are we just describing general differences, i.e., is estimated abundance just an index where higher values mean more fish than lower values.

See What I have learned (WHIL): data issues and also data summaries below.



## Consults {.tabset}
### Scott Reid
Talked to Scott Reid on 2024-10-16.  A very helpful and pleasant fellow.
I described my background, the Seal Cove situation where I took the estimates without question and then the Rose Blanche/Pamehac where I noticed large variances.  This promted me to dig deeper and look at the constant capture problem.  I then described using different methods and getting very different values including very improbable ones based on how capture probability is estimated.  Scott was sympathetic and that he had encountered all of these problems.  Indeed, he is so concerned that he has shelved some of his work with abundance and uses occupancy modeling because he's mostly worried about SAR.  He did say that salmonids are probably OK ito responding well to the electrofisher.

Tiered approach - Scott didn't baulk at this but did say it would have to be explained.   
Suggested:

- the importance of talking to the field staff and understanding issues  
- report a give method with worts or decide if the data is bad or decision tree  
- the importance of accuracy and precision - if  
- Consensus approach - do several approaches (or more) and see if they converge.  
- Bayesian - do a lit review and see what the catchabilities are.
- possibility of just going with counts?  


He shared concerns about pooling (see findings and solutions), i.e,. there are differences in size, shape, behaviour, and habitat (and also the time of year and differences in electrofishers).  He was especially concerned with flow because it probably interacts with all of the previous variables because it affects the distribution of the fish but also affects sampling.  Experience of the crews is very important too.

We discussed collaboration.  

### Paul Regular
2024-10-17 - Paul gave me almost 2 hours of his time.  I explained what I was up to and we went through the various iterations.  Paul wondered if the model was over paramaterized but was positive about the approach in general. 
We did come to the conclusion that the model is conflating q and n and is "blaming" q but not n.  Forcing B0 to be positive helps this.  So it would be good to know if the larger fish should be more catchable.  
We did establish that JAGS uses the greatest values as the intercept.

### Clarke and Curtis
2024-10-21  I outlined the issues that i've been having, i.e., that it all comes down to capture probability which changes for species, size, behaviour, and habitat/flow and that there is no silver bullet.  We discussed 4 solutions: CS/Bayesian/tiered and report the worts, pooling, and total catch.  The first two require a lot of explanation.  Clarke seems pretty confident that 3-pass electrofishing is catching most of the fish most of the time.  Curtis agreed.  We felt that this might be the more parsimonious approach.  

**They also pointed out that for Pamehac, the first years used a different electrofishing method**, i.e., the fish were driven downstream instead of being systematically fished upstream.

### Kristin Loughlin
2024-13-23: Outlined things with Kristin.  She made a lot of good suggestions about factors influencing the abundance (time of year, electorfishers).  We discussed pooling and issues with that.  She said that AS are more likely in the riffles and BT more so in the pools.  BT are also bigger in the pools.  

She asked a good question - does the YOY matter when we are really just 


## WHIL: approach
**Findings and solutions**
Generally, I think that the conclusion here is that you have to pick your poison and there are no silver bullets.  Also, far better to do capture-mark-recapture.  But from the WHIL section, clearly there are many sites with very few fish, the spc test often fails, and GF is often not met.  However, T ~ No in many cases, there are few cases when pass-4 or -5 will change things much, and if p > 0.5, T will be very close to No anyway.

Also, for Pamehac: 

- Few stations meet the T > 30 criteria in a given year  
- Some: CS approach is fine  
- Some: assumtions aren't met due to c2 > c1 or c1 ~ c2 >> c3 (spc/GF).
- Hedger approach seems to work in some cases but wildly off in others.  

GF > GF_crit is an indicator (although the power is probably low) but SPC needs to be looked at too.

A proposal for a tiered, measured approach (i.e., pick your poison):

- If T > 30 & GF < GF_crit and SPC looks OK (c1 > c2 > c3), then CS  
- If GF > GF_crit | SPC look bad, then  
    - if c1 << c2, then T (can't trust No and calibration will underestimate).  
        - if calibration >~ T and No, use calibration.  
        - if c1 ~ c2 >> c3, then T (can't trust No and calibration will overestimate)  
- If T < 30 & T > 20 same as above  
- If T < 20 & T > 10: same as above  
- If T < 10 - then T == No almost all the time but why use CS as its not really valid.  
 
Or, Just use T: see Clarke and Curtis meeting above. The thing is that if p > 0.5, then you get 87.5% of the fish.

### On pooling
I think I have always been a bit distrustful of the pooling approach for the reasons Scott and Kristin mentioned: different species, sizes, behaviours, habitats, flows, years, fishing crews, and time of year all suggest different catchabilities and that throwing everything in the same pot, and then multiplying the total by the proportion (without the delta method)  will somehow help. However, it did make sense to look at this just in case, so I did.  Pooling definitely helps with samples size and with spc.  However, there are still many cases when n < 30 (19/45 rows!) and about 12/45 where spc looks bad and 4/45 where GF > $GF_{crit}$ (I didn't check to see how many were in common).  So, while this improves things from the FSA:removal standpoint, we would still have lots of worts to explain plus how catchability is equal given all the above and then, delta methoding things appropriately.  Further, Scruton and Gibson [-scruton1995quantitative] explicitly recommend against this (pg. 109, 119 in pdf). I think let's give it a rest.

# Data summaries {.tabset}

## Stations

```{r}
with(df_all, table(Station, Sweep))
```

## by Stations-Species-Year
```{r}
with(df_all, table(Station, Sweep, Species, Year))
```


## samples & catch by year
```{r, message=FALSE}
library(kableExtra)
library(magrittr)
kbl(df_tab_T1, 
    col.names = c('row_num', 'Year', 'n', 'T', 'n', 'T', 'n', 'T', 'n', 'T'),
    align = 'c') |>
  kable_paper() |>
  # |>
  add_header_above(c(" " = 2, "AS" = 2, "ASYOY" = 2, "BT" = 2, "BTYOY" = 2))
  
```

# Analyses
See restoration Granite Readme.qmd, especially the What Have I Learned (WHIL)

Figs: Compare with what Kristin has done.  Kristin made a lot of figures but there's lots of redundancy.  See what is of value and reproduce.  Not much there - I have everything that I need.
I did the Density/biomass v Year with above and below as jittered CIs.
- Just do the all salmonids graph. Done
- need to supress x and y axis and legend as required for main fig. Done

## WHIL: data issues {.tabset}

### data summaries
```{r}
nrow(df_all) # number of fish caught
sum(rowSums(!is.na(df_tab3[,4:8]))) # number of sites sampled assuming 5 passes in 1990, 1991, 1996 and 3 in 1992 and 2016, including sweeps with abun == 0
nrow(df_tab3) # number of site:species:years including sweeps with abun == 0; assumes previous is true
nrow(df_a) # number of site:species:years that can be used in FSA
nrow(df_out)
```

### low n
```{r}

nrow(df_a |> filter(T < 30)) # 97 of 124
nrow(df_a |> filter(T < 20)) # 85 of 124
nrow(df_a |> filter(T < 10)) # 57 of 124
nrow(df_a |> filter(T < 5)) # 33 of 124

nrow(df_out |> filter(T < 30)) # 97 of 124
nrow(df_out |> filter(T < 20)) # 85 of 124
nrow(df_out |> filter(T < 10)) # 57 of 124
nrow(df_out |> filter(T < 5)) # 33 of 124
```

### spc
```{r, echo=FALSE}
p <- ggplot(df_sum_subset, 
  aes(x = spc, y = abun, 
    group = Station, fill = Station,
    text = paste("SPC: ", spc, "\n",
                 "Abund: ", abun, "\n",
                 "Stn: ", Station, "\n",
                 "Sweep: ", Sweep,
                 sep = "")
  )) +
  geom_point() +
  geom_path()

plotly::ggplotly(p, tooltip = "text")
```

### GF

```{r, echo=FALSE}
print("critical value")
qchisq(0.95, 1)
print("count of year:species:sites")
nrow(df_a)
#df_a |> filter(GF > qchisq(0.95, 1))
nrow(df_out)
df_out |> filter(GF > qchisq(0.95, 1))
```

### FSA: T v No


```{r, echo=FALSE}
ggplot(df_out, aes(x = T, y = No, group = as.factor(spp), colour = spp)) +
  geom_point()
```

### sites with 4-5 passes and fish

```{r, echo=F}
df_all |>
  group_by(Year, Species, Station) |>
  mutate(pass_no = ifelse(max(Sweep )<=3, 3, 5)) |>
  ungroup() |>
  filter(pass_no == 5) |> 
  group_by(Year, Species, Station, Sweep) |>
  summarize(count = n()) |>
  pivot_wider(names_from = Sweep, values_from = count, values_fill = 0) |>
  mutate(percent4_5 = (sum(`4`, `5`)/sum(`1`, `2`, `3`)*100), 
         percent4 = (sum(`4`)/sum(`1`, `2`, `3`)*100),
         diff = sum(`1`, `2`, `3`) - sum(`1`, `2`, `3`, `4`)) |>
  relocate(`1`, .after = Station)
```

### p v T
```{r, echo=FALSE}
n <- 100
p <- 0.5

c1 <- p*n
c2 <- (n-c1)*p
c3 <- (n- c1-c2)*p
T <- sum(c1, c2, c3)

p_vec <- seq(0.01, 1, 0.01)
T_vec <- rep(NA, length(p_vec))

i <- 2
for(i in seq_along(p_vec)){
  c1 <- p_vec[i]*n
  c2 <- (n-c1)*p_vec[i]
  c3 <- (n- c1-c2)*p_vec[i]
  T_vec[i] <- sum(c1, c2, c3)
}

plot(p_vec, T_vec, type = 'n')
lines(p_vec, T_vec)
abline(h=90)
```


Perhaps graph the 4-5 pass sites - its a key bit of evidences


# References

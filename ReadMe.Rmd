---
title: "ReadMe - Pamehac"
author: "Keith Lewis"
date: "2024-08-07"
output: html_document
bibliography: refs/references.bib
---

# Introduction
This file is simply a record of issues that I have had with the Pamehac analysis that was started by Kristin Loughlin and is a continuation of work begun by Dave Scruton.   See ReadMe files from Rose Blanche and Seal Cove for additional info.

# Notes

Got a zip file from Kristin Loughlin with no folders but files as follows:  
- Carle Strub Estimates    
- data in various formats  
- some R files for various plots  

I reorganized for my own needs.  I have recreated all teh figures although there are still questions, especially about the confidence intervals.



# Data
See the Rose Blanche code for explanations on what I think Kristin did and my concerns, i.e., how to use the delta method for tehse types of data as well as how the initial summaries were naively done.  
BUT......the really, really, really good thing for Pamehac is that there is a file "output_pamehac_by_station.csv" that has all of the data as derived from the AMEC functions but in raw form!!!  

Need a function to do this?


## Why are some CIs wide
Several reasons:
1. lack of data  - if only a few fish, often get NA
2. lack of depletion

On a more minor note:  
Are there any diagnostics or other concerns with the CS method?  
Why does the AMEC function not return its output aside from a graph and a csv file???

## abundance estimate - literature
After extensive exploration, it is clear that there is no "right" way to model abundance for all of the species:year:stations.  This is consistent with Riley and Fausch [-@riley1992underestimation] who found "decreasing capture probability was a frequent problem when estimating trout abundance in small Colorado mountain streams".  Their streams seem comparable to Seal Cove.

Cowx [-@cowx1983review] showed that Carle Strub is the more reliable method than earlier approaches such as Zippin. Has the equations, much like the tutorial by Lockwood [-@lockwood2000stream].  Also lists 5 assumptions of depletion methods.  Gives the downsides of all methods except CS.

Gatz and Loar [-@gatz1988petersen] show that results can be improved by combining Petersen with removal methods in a reasonable amount of time, i.e., 2 days

Riley and Fausch [-@riley1992underestimation] found that 2 and 3-pass estimates result in underestimates but 3 is better than 2.  Suggest that its due to decreasing p but not complexity of habitat but I found their statistical methods wanting on this front.  But the power of chi-square tests is low below population size of 200. Therefore, its important to look at chi-square and spc together.  

Heimbuch et al. [-@doublepass] use a double pass approach but I think that this is a bit like Hedger - they use all sites to get an approximation of catchability - I don't think this is a good idea.

Lockwood [-@lockwood2000stream] is a great tutorial on recapture and depletion methods.  See also 2013 fishR tutorial but Derek Ogle.

Reid [-@reid2009evaluation] - single pass seems to work OK as an index; is this OK for GLMMs?
Reid 2011 - eels

Hedger et al. [-@hedger2013improving] found that - summarize constant $\rho$ based on first pass and variable p based on 

Matechou 2016 (not in refs) - open models for removal - cool but to be read later

Reid and Haxton [-@reid2017backpack] used a Bayesian approach to model sampling data, site occupancy, and detection probability.  I think that this is a state-space (N-mixture) approach because they model occupancy and then use that in the model of observations. No covariates to model detection. They also looked at time spent fishing.

Hanks 2018 - differences in power bewteen 1- and 3-passses to detect annual population declines diminishes as more sites are surveyed and nearly identical when 5+ sites are surveyed but methods are very correlated. Focus is on trend detection.  But single pass is an index, not absolute abundance. can't compare with other areas with different capture probabilities.  

Hedger 2018 - capture probability estimates depend on number of passes and decline with each pass with effect depending on size class - probably not a surprise. Ergo, 3-pass will underestimate.

Link 2018 (not in refs) robust design and removal.  Looks cool but may not apply and will be hard to implement

Rivera 2021 - a review: very general but with no key insights.  It does provide a nice history of the field.  Almost all depletion can be treated as a CMR analysis but with recapture set to zero.

Richter et al [-@richter2022correcting] use a Bayesian method to correct for size bias.  There is a lot of merit in this approach - i'm just not sure if it really makes that much difference in the end.  They also have some rules for when to stop sampling but also which reaches to include in the data set.  But it seems like they are just doing a standard depletion model with q as catchability - they are not trying to model q in anyway through a regression.  Should explore. https://github.com/ianarichter/bayesian-removal-model

Lamothe et al. 2023 - simulations on occupancy and n-mixture; occupancy models are slightly better.  Fig.1 is great - both are state-space models but occupancy models occupancy before observed number while n-mixture models true abundance before observed abundance. Finish reading. They used gmultmix - need to figure out why this and not multinomPois
https://cran.rstudio.com/web/packages/unmarked/vignettes/cap-recap.html

Lecture10: https://rushinglab.github.io/WILD6900/articles/Lecture10/Lecture10.html#4

What have I learned:
- Assumptions [@cowx1983review]: 3 standard and two more  
    - closed population - this should always be met in NL
    - probability of capture is the same for all individuals  
    - probability of capture (catchability) remains the same for each capture occasion - unlikely to be true often
    - non-standard: the CPUE must significantly reduce the population size  
    - non-standard: the population was not be so large that catching of one member
interferes with the capture of another  

What about this??? 2. fishing effort must remain constant for each capture occasion - assume that this is the case but see Clarke and Curtis


- Can do depletion on open and closed  
- Samples > 30 total are required  
- Goodness-of-fit power isn't great with < 200 fish  
- Have to capture most of the fish  
- Assumptions are often violated  

I think that the conclusion here is that you have to pick your poison.  Lots of passes means less sites, less power, and less generalization.  1 pass means more sites but less accuracy of the estimate.  But I want to confirm the 'pick your poison' with Reid and Ricther.  I also want to know if this really makes any difference or are we just describing general differences, i.e., is estimated abundance just an index where higher values mean more fish than lower values.

However: there is package unmarked (Finske and Chandler 2011) and the Bayesian spinoffs.  
"With the exception of the `open' population models (colext, gmultmix, and pcountOpen), all fitting functions use a double right-hand sided formula syntax representing the hierarchical model structure. Specifically, covariates
affecting the **detection process** are specified following the first tilde, and covariates of the **state process** follow the second tilde. **No left-hand side** of the formula is specified because the unmarked Frame defines the response variable uniquely as the y slot."
"As described in Section 2.2, covariates of detection
are on the logit-scale and covariates of abundance are on the log-scale for the repeated count
model."

## abundance estimate - findings and solutions
Few stations meet the T > 30 criteria in a given year
Some: CS approach is fine
Some: assumtions aren't met due to c2 > c1 or c1 ~ c2 >> c3.
Hedger approach seems to work in some cases but wildly off in others.  

GF > GF_crit is an indicator (although the power is probably low) but SPC needs to be looked at too.

Need a tiered, measured approach
 - If T > 30 & GF < GF_crit and SPC looks OK (c1 > c2 > c3), then CS
    - If GF > GF_crit | SPC look bad, then 
      - if c1 << c2, then T (can't trust No and calibration will underestimate).  
      - if calibration >~ T and No, use calibration.  
      - if c1 ~ c2 >> c3, then T (can't trust No and calibration will overestimate)  
 - If T < 30 & T > 20 same as above
 - If T < 20 & T > 10: same as above
 - If T < 10 - then T == No almost all the time but why use CS as its not really valid.
 



# References

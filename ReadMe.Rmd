---
title: "ReadMe - Pamehac"
author: "Keith Lewis"
date: "2024-08-07"
output: html_document
bibliography: refs/references.bib
---

```{r, echo = F, message = F}
df_all <- read.csv("derived_data/df_all.csv")
df_sum <- read.csv("derived_data/df_sum.csv")
df_tab_T <- read.csv("derived_data/df_tab_T.csv")
df_sum_subset <- read.csv("derived_data/spc_example.csv")
df_out <- read.csv("derived_data/FSA_output.csv")

library(ggplot2)
library(dplyr)
library(tidyr)
```

# Introduction
This file is simply a record of issues that I have had with the Pamehac analysis that was started by Kristin Loughlin and is a continuation of work begun by Dave Scruton.   See ReadMe files from Rose Blanche and Seal Cove for additional info.

# Notes

Got a zip file from Kristin Loughlin with no folders but files as follows:  
- Carle Strub Estimates    
- data in various formats  
- some R files for various plots  

I reorganized for my own needs.  I have recreated all the figures although there are still questions, especially about the confidence intervals so basically, I learved how to do Carle Strub and many other estimates of abundance.

## Missing values
2024-10-23: checked that df_all, df_sum, df_tab1, and out all add up.  df_all and df_sum are fine.  df_tab1 can differ from the less refined dataframes because df_tab1 includes only Sweep 1 to Sweep3, originally eliminated sites with only Sweep 1, and then those where Sweeps 2 and 3 were NA.

I think that this is OK for analytical purposes but there may be troubles with the tabulation.

# Data
See the Rose Blanche code for explanations on what I think the AMEC code does and my concerns, i.e., how to use the delta method for tehse types of data as well as how the initial summaries were naively done.  
BUT......the really, really, really good thing for Pamehac is that there is a file "output_pamehac_by_station.csv" that has all of the data as derived from the AMEC functions but in raw form!!!  So, I have created code in Pam_data_new.R to bring in all the files together.  From there, one can do any type of depletion analysis.


# Abundance Estimates
## Why are some CIs wide
One thing that I noticed though is that the CIs for some of the estimates are very narrow while others are extremely wide.  Why are they so wide?
Several reasons:  
1. lack of data  - if only a few fish, often get NA  
2. lack of depletion  

On a more minor note:  
See below for assumptions behind depletion method and Pam_data_new for "Sum of Previous Catch" (spc) and Pam_abun_bio.R for Goodness of fit (GF) tests.  

Why does the AMEC function not return its output aside from a graph and a csv file???

## literature review {.tabset}

After reviewing the early (~ pre-2000) literature on depletion estimates, it became clear that the CS method is best but there is a whole new literature composed of N-mixture models and Bayesian approaches.  

### early
After extensive exploration, it is clear that there is no "right" way to model abundance for all of the species:year:stations.  This is consistent with Riley and Fausch [-@riley1992underestimation] who found "decreasing capture probability was a frequent problem when estimating trout abundance in small Colorado mountain streams".  Their streams seem comparable to Seal Cove.

Cowx [-@cowx1983review] showed that Carle Strub is the more reliable method than earlier approaches such as Zippin. Has the equations, much like the tutorial by Lockwood [-@lockwood2000stream].  Also lists 5 assumptions of depletion methods.  Gives the downsides of all methods except CS.

Gatz and Loar [-@gatz1988petersen] show that results can be improved by combining Petersen with removal methods in a reasonable amount of time, i.e., 2 days

Riley and Fausch [-@riley1992underestimation] found that 2 and 3-pass estimates result in underestimates but 3 is better than 2.  Suggest that its due to decreasing p but not complexity of habitat but I found their statistical methods wanting on this front.  But the power of chi-square tests is low below population size of 200. Therefore, its important to look at chi-square and spc together.  

### later (post 2000)
Heimbuch et al. [-@doublepass] use a double pass approach but I think that this is a bit like Hedger - they use all sites to get an approximation of catchability - I don't think this is a good idea.

Reid [-@reid2009evaluation] - single pass seems to work OK as an index; is this OK for GLMMs?
Reid 2011 - eels

Hedger et al. [-@hedger2013improving] found that - summarize constant $\rho$ based on first pass and variable p based on 

Matechou 2016 (not in refs) - open models for removal - cool but to be read later

Hanks 2018 - differences in power bewteen 1- and 3-passses to detect annual population declines diminishes as more sites are surveyed and nearly identical when 5+ sites are surveyed but methods are very correlated. Focus is on trend detection.  But single pass is an index, not absolute abundance. can't compare with other areas with different capture probabilities.  

Hedger 2018 - capture probability estimates depend on number of passes and decline with each pass with effect depending on size class - probably not a surprise. Ergo, 3-pass will underestimate.

Link 2018 (not in refs) robust design and removal.  Looks cool but may not apply and will be hard to implement

### Bayesian
Working on the Richter approach - it seems fine conceptually but suffers from the same problem as package unmarked, i.e., it all comes down to q.  When I set the q, it seems to come up with reasonable results. See Paul Regular below.   

So, I think I found a clue to why the population estimates are so high.  If you do the length approach advocated by Richter, then there are some lengths where there is no depletion and this would suggest that there are lots more fish.  But for very low n(n <=3, the q for the second length is still very, very low).

Reid and Haxton [-@reid2017backpack] used a Bayesian approach to model sampling data, site occupancy, and detection probability.  I think that this is a state-space (N-mixture) approach because they model occupancy and then use that in the model of observations. No covariates to model detection. They also looked at time spent fishing.

Richter et al [-@richter2022correcting] use a Bayesian method to correct for size bias.  There is a lot of merit in this approach - i'm just not sure if it really makes that much difference in the end.  They also have some rules for when to stop sampling but also which reaches to include in the data set.  But it seems like they are just doing a standard depletion model with q as catchability - they are not trying to model q in anyway through a regression.  Explored this in detail - it still gives wonky estimates because q can be very low.  This may or may not be because of non-declining catches or low sample sizes.  Don't understand how q is estimated based on site and size, especially for all three removal periods. https://github.com/ianarichter/bayesian-removal-model

### N-mixture/unmarked
However: there is package unmarked (Finske and Chandler 2011) and the Bayesian spinoffs.  
"With the exception of the `open' population models (colext, gmultmix, and pcountOpen), all fitting functions use a double right-hand sided formula syntax representing the hierarchical model structure. Specifically, covariates
affecting the **detection process** are specified following the first tilde, and covariates of the **state process** follow the second tilde. **No left-hand side** of the formula is specified because the unmarked Frame defines the response variable uniquely as the y slot."
"As described in Section 2.2, covariates of detection
are on the logit-scale and covariates of abundance are on the log-scale for the repeated count
model."

Conclusion on umarked:
Spent a lot of time on package unmarked but there are very few tutorials and everything is dependent on the 'q' estimate which can result in wildly non-sensical estimates

Lamothe et al. 2023 - simulations on occupancy and n-mixture; occupancy models are slightly better.  Fig.1 is great - both are state-space models but occupancy models occupancy before observed number while n-mixture models true abundance before observed abundance. Finish reading. They used gmultmix - need to figure out why this and not multinomPois
https://cran.rstudio.com/web/packages/unmarked/vignettes/cap-recap.html


### tutorials/reviews
Lockwood [-@lockwood2000stream] is a great tutorial on recapture and depletion methods.  See also 2013 fishR tutorial but Derek Ogle.

Lecture10: https://rushinglab.github.io/WILD6900/articles/Lecture10/Lecture10.html#4

Rivera 2021 - a review: very general but with no key insights.  It does provide a nice history of the field.  Almost all depletion can be treated as a CMR analysis but with recapture set to zero.


## What have I learned:
Assumptions [@cowx1983review]: 3 standard and two more:    
    - closed population -> this should always be met in NL
    - probability of capture is the same for all individuals  
    - probability of capture (catchability) remains the same for each capture occasion - unlikely to be true often
    - non-standard: the CPUE must significantly reduce the population size  
    - non-standard: the population was not be so large that catching of one member
interferes with the capture of another  

What about this??? 2. fishing effort must remain constant for each capture occasion - assume that this is the case but see Clarke and Curtis

- Can do depletion on open and closed  
- Samples > 30 total are required  
- Goodness-of-fit power isn't great with < 200 fish  
- Have to capture most of the fish (but this begs the question of why employ these approaches to begin with)
- Assumptions are often violated  

Lots of passes means less sites, less power, and less generalization.  1 pass means more sites but less accuracy of the estimate.  I decided that it was time to reach out (see Consults).  I also want to know if this really makes any difference or are we just describing general differences, i.e., is estimated abundance just an index where higher values mean more fish than lower values.

See What I have learned (WHIL): data issues and also data summaries below.


## WHIL: data issues {.tabset}
### low n
```{r}
nrow(df_all) # number of fish caught
nrow(df_sum) # number of sites sampled even if no fish
nrow(df_out) # number of site:years with fish
nrow(df_out |> filter(T < 30)) # 97 of 124
nrow(df_out |> filter(T < 20)) # 85 of 124
nrow(df_out |> filter(T < 10)) # 57 of 124
nrow(df_out |> filter(T < 5)) # 33 of 124
```

### spc
```{r, echo=FALSE}
p <- ggplot(df_sum_subset, 
  aes(x = spc, y = abun, 
    group = Station, fill = Station,
    text = paste("SPC: ", spc, "\n",
                 "Abund: ", abun, "\n",
                 "Stn: ", Station, "\n",
                 "Sweep: ", Sweep,
                 sep = "")
  )) +
  geom_point() +
  geom_path()

plotly::ggplotly(p, tooltip = "text")
```

### GF

```{r, echo=FALSE}
print("critical value")
qchisq(0.95, 1)
print("count of year:species:sites")
nrow(df_out)
df_out |> filter(GF > qchisq(0.95, 1))
```

### FSA: T v No
```{r, echo=FALSE}
ggplot(df_out, aes(x = T, y = No, group = as.factor(spp), colour = spp)) +
  geom_point()
```

### sites with 4-5 passes and fish

```{r, echo=F}
df_all |>
  group_by(Year, Species, Station) |>
  mutate(pass_no = ifelse(max(Sweep )<=3, 3, 5)) |>
  ungroup() |>
  filter(pass_no == 5) |> 
  group_by(Year, Species, Station, Sweep) |>
  summarize(count = n()) |>
  pivot_wider(names_from = Sweep, values_from = count, values_fill = 0)
```

### p v T
```{r, echo=FALSE}
n <- 100
p <- 0.5

c1 <- p*n
c2 <- (n-c1)*p
c3 <- (n- c1-c2)*p
T <- sum(c1, c2, c3)

p_vec <- seq(0.01, 1, 0.01)
T_vec <- rep(NA, length(p_vec))

i <- 2
for(i in seq_along(p_vec)){
  c1 <- p_vec[i]*n
  c2 <- (n-c1)*p_vec[i]
  c3 <- (n- c1-c2)*p_vec[i]
  T_vec[i] <- sum(c1, c2, c3)
}

plot(p_vec, T_vec, type = 'n')
lines(p_vec, T_vec)
abline(h=90)
```



## Consults {.tabset}
### Scott Reid
Talked to Scott Reid on 2024-10-16.  A very helpful and pleasant fellow.
I described my background, the Seal Cove situation where I took the estimates without question and then the Rose Blanche/Pamehac where I noticed large variances.  This promted me to dig deeper and look at the constant capture problem.  I then described using different methods and getting very different values including very improbable ones based on how capture probability is estimated.  Scott was sympathetic and that he had encountered all of these problems.  Indeed, he is so concerned that he has shelved some of his work with abundance and uses occupancy modeling because he's mostly worried about SAR.  He did say that salmonids are probably OK ito responding well to the electrofisher.

Tiered approach - Scott didn't baulk at this but did say it would have to be explained.   
Suggested:

- the importance of talking to the field staff and understanding issues  
- report a give method with worts or decide if the data is bad or decision tree  
- the importance of accuracy and precision - if  
- Consensus approach - do several approaches (or more) and see if they converge.  
- Bayesian - do a lit review and see what the catchabilities are.
- possibility of just going with counts?  


He shared concerns about pooling, i.e,. there are differences in size, shape, behaviour, and habitat (and also the time of year and differences in electrofishers).  He was especially concerned with flow because it probably interacts with all of the previous variables because it affects the distribution of the fish but also affects sampling.  Experience of the crews is very important too.

We discussed collaboration.  

### Paul Regular
2024-10-17 - Paul gave me almost 2 hours of his time.  I explained what I was up to and we went through the various iterations.  Paul wondered if the model was over paramaterized but was positive about the approach in general. 
We did come to the conclusion that the model is conflating q and n and is "blaming" q but not n.  Forcing B0 to be positive helps this.  So it would be good to know if the larger fish should be more catchable.  
We did establish that JAGS uses the greatest values as the intercept.

### Clarke and Curtis
2024-10-21  I outlined the issues that i've been having, i.e., that it all comes down to capture probability which changes for species, size, behaviour, and habitat/flow and that there is no silver bullet.  We discussed 4 solutions: CS/Bayesian/tiered and report the worts, pooling, and total catch.  The first two require a lot of explanation.  Clarke seems pretty confident that 3-pass electrofishing is catching most of the fish most of the time.  Curtis agreed.  We felt that this might be the more parsimonious approach.  

**They also pointed out that for Pamehac, the first years used a different electrofishing method**, i.e., the fish were driven downstream instead of being systematically fished upstream.

### Kristin Loughlin
2024-13-23: Outlined things with Kristin.  She made a lot of good suggestions about factors influencing the abundance (time of year, electorfishers).  We discussed pooling and issues with that.  She said that AS are more likely in the riffles and BT more so in the pools.  BT are also bigger in the pools.  

She asked a good question - does the YOY matter when we are really just 


## findings and solutions
Generally, I think that the conclusion here is that you have to pick your poison and there are no silver bullets.  Also, far better to do capture-mark-recapture.

For Pamehac: 

- Few stations meet the T > 30 criteria in a given year  
- Some: CS approach is fine  
- Some: assumtions aren't met due to c2 > c1 or c1 ~ c2 >> c3 (spc/GF).
- Hedger approach seems to work in some cases but wildly off in others.  

GF > GF_crit is an indicator (although the power is probably low) but SPC needs to be looked at too.

A proposal for a tiered, measured approach (i.e., pick your poison):

- If T > 30 & GF < GF_crit and SPC looks OK (c1 > c2 > c3), then CS  
- If GF > GF_crit | SPC look bad, then  
    - if c1 << c2, then T (can't trust No and calibration will underestimate).  
        - if calibration >~ T and No, use calibration.  
        - if c1 ~ c2 >> c3, then T (can't trust No and calibration will overestimate)  
- If T < 30 & T > 20 same as above  
- If T < 20 & T > 10: same as above  
- If T < 10 - then T == No almost all the time but why use CS as its not really valid.  
 
Or, Just use T: see Clarke and Curtis meeting above. The thing is that if p > 0.5, then you get 87.5% of the fish.


## Data summaries {.tabset}

### Stations

```{r}
with(df_all, table(Station, Sweep))
```

### by Stations-Species-Year
```{r}
with(df_all, table(Station, Sweep, Species, Year))
```


### samples & catch by year
```{r, echo=F}
# the first column is the row numbers - need to fix this
library(kableExtra)
library(magrittr)
kbl(df_tab_T, 
    col.names = c('x', 'Year', 'n', 'T', 'n', 'T', 'n', 'T', 'n', 'T'),
    align = 'c') |>
  kable_paper() |>
  # |>
  add_header_above(c(" " = 2, "AS" = 2, "ASYOY" = 2, "BT" = 2, "BTYOY" = 2))
  
```


# References
